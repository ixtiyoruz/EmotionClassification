
logging:
    use_wandb: True

    wandb:
        project: 'Emotions'
        group: 'general'

general:
    classes: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
    n_workers: 4
    train_print_frequency: 100
    valid_print_frequency: 50

    train_batch_size: 32
    valid_batch_size: 32

    seed: 42

    max_length: 1024
    set_max_length_from_data: True
    log_path: "./logs"
    model_fn_path: "./checkpoints"
    tokenizer_dir_path: './tokenizer'


model:
    backbone_type: 'microsoft/deberta-v3-base'
    pretrained_backbone: True
    from_checkpoint: False
    checkpoint_id: ''
    classifier_dropout: 0.5
    backbone_config_path: ''
    backbone_hidden_dropout: 0.
    backbone_hidden_dropout_prob: 0.
    backbone_attention_dropout: 0.
    backbone_attention_probs_dropout_prob: 0.

    pooling_type: 'MeanPooling' # ['MeanPooling', 'ConcatPooling', 'WeightedLayerPooling', 'GRUPooling', 'LSTMPooling', 'AttentionPooling']

    gru_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False

    weighted_pooling:
        layer_start: 4
        layer_weights: null

    wk_pooling:
        layer_start: 4
        context_window_size: 2

    lstm_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False
        
    attention_pooling:
        hiddendim_fc: 1024
        dropout: 0.1
        
    concat_pooling:
        n_layers: 4

    gradient_checkpointing: True

    freeze_embeddings: False
    freeze_n_layers: 0
    reinitialize_n_layers: 1


optimizer:
    encoder_lr: 0.00001
    embeddings_lr: 0.00001
    decoder_lr: 0.00001

    eps: 1.e-6
    betas: [0.9, 0.999]

    weight_decay: 0.01

scheduler:
    scheduler_type: 'cosine_schedule_with_warmup' # [constant_schedule_with_warmup, linear_schedule_with_warmup, cosine_schedule_with_warmup,polynomial_decay_schedule_with_warmup]
    batch_scheduler: True

    constant_schedule_with_warmup:
        n_warmup_steps: 0

    linear_schedule_with_warmup:
        n_warmup_steps: 0

    cosine_schedule_with_warmup:
        n_cycles: 0.5
        n_warmup_steps: 0

    polynomial_decay_schedule_with_warmup:
        n_warmup_steps: 0
        power: 1.0
        min_lr: 0.0

adversarial_learning:
    adversarial_lr: 0.0001
    adversarial_eps: 0.005
    adversarial_epoch_start: 99

training:
    epochs: 20
    apex: True
    max_grad_norm: 1000
    unscale: False

    
criterion:
    criterion_type: 'cross_entropy'
    cross_entropy:
      weights: null